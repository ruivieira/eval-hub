provider_id: ragas
provider_name: RAGAS
description: Retrieval Augmented Generation Assessment framework
provider_type: builtin
base_url: null
runtime:
  k8s:
    image: quay.io/eval-hub/ragas:latest
    entrypoint:
      - python
      - /opt/app-root/src/main.py
    cpu_request: 100m
    memory_request: 128Mi
    cpu_limit: 500m
    memory_limit: 1Gi
    env_vars:
      # Default environment variables
      VAR_NAME: VALUE
  local:
    # reserved for local runtime

benchmarks:
  - benchmark_id: faithfulness
    name: Faithfulness
    description: Measures factual consistency of generated answer against given context
    category: rag_quality
    metrics:
      - faithfulness_score
    num_few_shot: 0
    dataset_size: null
    tags:
      - rag
      - faithfulness
      - factuality
  - benchmark_id: answer_relevancy
    name: Answer Relevancy
    description: Measures how relevant generated answer is to the question
    category: rag_quality
    metrics:
      - answer_relevancy_score
    num_few_shot: 0
    dataset_size: null
    tags:
      - rag
      - relevancy
      - quality
  - benchmark_id: context_precision
    name: Context Precision
    description: Measures precision of retrieved context
    category: rag_retrieval
    metrics:
      - context_precision_score
    num_few_shot: 0
    dataset_size: null
    tags:
      - rag
      - retrieval
      - precision
  - benchmark_id: context_recall
    name: Context Recall
    description: Measures recall of retrieved context
    category: rag_retrieval
    metrics:
      - context_recall_score
    num_few_shot: 0
    dataset_size: null
    tags:
      - rag
      - retrieval
      - recall
