{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Eval Hub API Examples\"\n",
    "subtitle: \"Comprehensive guide to using the Evaluation Hub REST API\"\n",
    "author: \"Evaluation Service Team\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    code-fold: false\n",
    "    theme: cosmo\n",
    "  ipynb:\n",
    "    output-file: api_examples.ipynb\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "# Eval Hub API Examples\n",
    "\n",
    "This notebook demonstrates how to interact with the Evaluation Hub REST API running on `localhost:8000`.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from uuid import uuid4\n",
    "\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "API_BASE = f\"{BASE_URL}/api/v1\"\n",
    "\n",
    "# Helper function for pretty printing JSON responses\n",
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2, default=str))\n",
    "\n",
    "# Helper function for API requests\n",
    "def api_request(method: str, endpoint: str, **kwargs) -> requests.Response:\n",
    "    \"\"\"Make an API request with proper error handling.\"\"\"\n",
    "    url = f\"{API_BASE}{endpoint}\"\n",
    "    response = requests.request(method, url, **kwargs)\n",
    "\n",
    "    print(f\"{method.upper()} {url}\")\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "\n",
    "    if response.headers.get('content-type', '').startswith('application/json'):\n",
    "        print(\"Response:\")\n",
    "        print_json(response.json())\n",
    "    else:\n",
    "        print(f\"Response: {response.text}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "\n",
    "First, let's verify the service is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/health\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    health_data = response.json()\n",
    "    print(\"‚úÖ Service is healthy!\")\n",
    "    print(f\"Version: {health_data['version']}\")\n",
    "    print(f\"Uptime: {health_data['uptime_seconds']:.1f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå Service is not responding correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider Management\n",
    "\n",
    "### List All Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/providers\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    providers_data = response.json()\n",
    "    print(f\"Found {providers_data['total_providers']} providers:\")\n",
    "    for provider in providers_data['providers']:\n",
    "        print(f\"  - {provider['provider_name']} ({provider['provider_id']})\")\n",
    "        print(f\"    Type: {provider['provider_type']}\")\n",
    "        print(f\"    Benchmarks: {provider['benchmark_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Specific Provider Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details for the lm_evaluation_harness provider\n",
    "provider_id = \"lm_evaluation_harness\"\n",
    "response = api_request(\"GET\", f\"/providers/{provider_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    provider = response.json()\n",
    "    print(f\"Provider: {provider['provider_name']}\")\n",
    "    print(f\"Description: {provider['description']}\")\n",
    "    print(f\"Number of benchmarks: {len(provider['benchmarks'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Discovery\n",
    "\n",
    "### List All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/benchmarks\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    benchmarks_data = response.json()\n",
    "    print(f\"Total benchmarks available: {benchmarks_data['total_count']}\")\n",
    "\n",
    "    # Show first 5 benchmarks\n",
    "    for benchmark in benchmarks_data['benchmarks'][:5]:\n",
    "        print(f\"  - {benchmark['name']} ({benchmark['benchmark_id']})\")\n",
    "        print(f\"    Category: {benchmark['category']}\")\n",
    "        print(f\"    Provider: {benchmark['provider_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Benchmarks by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/benchmarks\", params={\"category\": \"math\"})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    math_benchmarks = response.json()\n",
    "    print(f\"Math benchmarks: {math_benchmarks['total_count']}\")\n",
    "    for benchmark in math_benchmarks['benchmarks']:\n",
    "        print(f\"  - {benchmark['name']}: {benchmark['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Provider-Specific Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_id = \"lm_evaluation_harness\"\n",
    "response = api_request(\"GET\", f\"/providers/{provider_id}/benchmarks\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    benchmarks = response.json()\n",
    "    print(f\"Benchmarks for {provider_id}: {len(benchmarks)}\")\n",
    "\n",
    "    # Group by category\n",
    "    categories = {}\n",
    "    for benchmark in benchmarks:\n",
    "        category = benchmark['category']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        categories[category].append(benchmark['name'])\n",
    "\n",
    "    for category, names in categories.items():\n",
    "        print(f\"\\n{category.title()}: {len(names)} benchmarks\")\n",
    "        print(f\"  Examples: {', '.join(names[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections\n",
    "\n",
    "### List Available Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/collections\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collections = response.json()\n",
    "    print(f\"Available collections: {collections['total_collections']}\")\n",
    "\n",
    "    for collection in collections['collections']:\n",
    "        print(f\"\\nüìÅ {collection['name']} ({collection['collection_id']})\")\n",
    "        print(f\"   Description: {collection['description']}\")\n",
    "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
    "        for benchmark_ref in collection['benchmarks'][:3]:  # Show first 3\n",
    "            print(f\"     - {benchmark_ref['provider_id']}::{benchmark_ref['benchmark_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection of available lm-evaluation-harness benchmarks for coding and reasoning evaluation\n",
    "coding_reasoning_collection = {\n",
    "    \"collection_id\": \"coding_reasoning_v1\",\n",
    "    \"name\": \"Coding & Reasoning Collection v1\",\n",
    "    \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
    "    \"tags\": [\"coding\", \"reasoning\", \"v1\"],\n",
    "    \"benchmarks\": [\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"arc_easy\",\n",
    "            \"weight\": 1.5,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 25,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"humaneval\",\n",
    "            \"weight\": 2.0,  # Higher weight for coding benchmark\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 50\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"mbpp\",\n",
    "            \"weight\": 2.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 50\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"bbh\",\n",
    "            \"weight\": 1.5,  # Big-bench hard for reasoning\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 3,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"created_by\": \"evaluation_team\",\n",
    "        \"use_case\": \"coding_reasoning_assessment\",\n",
    "        \"difficulty\": \"intermediate_to_hard\",\n",
    "        \"estimated_duration_minutes\": 30\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating custom coding & reasoning collection...\")\n",
    "print_json(coding_reasoning_collection)\n",
    "\n",
    "response = api_request(\"POST\", \"/collections\", json=coding_reasoning_collection)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    created_collection = response.json()\n",
    "    print(\"‚úÖ Collection created successfully!\")\n",
    "    print(f\"Collection ID: {created_collection['collection_id']}\")\n",
    "    print(f\"Total benchmarks: {len(created_collection['benchmarks'])}\")\n",
    "    print(f\"Created at: {created_collection.get('created_at', 'N/A')}\")\n",
    "\n",
    "    # Store collection ID for later use\n",
    "    coding_reasoning_collection_id = created_collection['collection_id']\n",
    "else:\n",
    "    print(f\"‚ùå Failed to create collection: {response.text}\")\n",
    "    coding_reasoning_collection_id = \"coding_reasoning_v1\"  # Fallback for examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Language Understanding Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection focused on language understanding and modeling\n",
    "language_collection = {\n",
    "    \"collection_id\": \"language_understanding_v1\",\n",
    "    \"name\": \"Language Understanding Collection v1\",\n",
    "    \"description\": \"Collection of language modeling and comprehension benchmarks\",\n",
    "    \"tags\": [\"language\", \"understanding\", \"comprehension\"],\n",
    "    \"benchmarks\": [\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"lambada_openai\",\n",
    "            \"weight\": 1.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 200\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"blimp\",\n",
    "            \"weight\": 1.5,  # Grammar and linguistic knowledge\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"arc_easy\",  # For basic reasoning\n",
    "            \"weight\": 1.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 25,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"created_by\": \"nlp_team\",\n",
    "        \"use_case\": \"language_understanding_assessment\",\n",
    "        \"difficulty\": \"beginner_to_intermediate\",\n",
    "        \"focus_areas\": [\"language_modeling\", \"grammar\", \"comprehension\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating language understanding collection...\")\n",
    "response = api_request(\"POST\", \"/collections\", json=language_collection)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    language_collection_id = response.json()['collection_id']\n",
    "    print(f\"‚úÖ Language collection created: {language_collection_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Language collection creation failed (may already exist)\")\n",
    "    language_collection_id = \"language_understanding_v1\"  # Fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Collections (Including New Ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the collections list to see our new collections\n",
    "response = api_request(\"GET\", \"/collections\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collections = response.json()\n",
    "    print(f\"üìÅ Total collections available: {collections['total_collections']}\")\n",
    "\n",
    "    # Show all collections with details\n",
    "    for collection in collections['collections']:\n",
    "        print(f\"\\nüìÅ {collection['name']}\")\n",
    "        print(f\"   ID: {collection['collection_id']}\")\n",
    "        print(f\"   Provider: {collection['provider_id']}\")\n",
    "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
    "        print(f\"   Tags: {', '.join(collection.get('tags', []))}\")\n",
    "        if collection.get('metadata', {}).get('difficulty'):\n",
    "            print(f\"   Difficulty: {collection['metadata']['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Specific Collection Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about our coding & reasoning collection\n",
    "collection_id = coding_reasoning_collection_id\n",
    "response = api_request(\"GET\", f\"/collections/{collection_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collection = response.json()\n",
    "    print(f\"üìã Collection: {collection['name']}\")\n",
    "    print(f\"Description: {collection['description']}\")\n",
    "    print(f\"Provider: {collection['provider_id']}\")\n",
    "\n",
    "    print(f\"\\nüéØ Benchmarks ({len(collection['benchmarks'])}):\")\n",
    "    total_weight = sum(b.get('weight', 1.0) for b in collection['benchmarks'])\n",
    "\n",
    "    for benchmark in collection['benchmarks']:\n",
    "        weight = benchmark.get('weight', 1.0)\n",
    "        weight_pct = (weight / total_weight) * 100\n",
    "        print(f\"  - {benchmark['benchmark_id']} (weight: {weight}, {weight_pct:.1f}%)\")\n",
    "        if benchmark.get('config'):\n",
    "            config = benchmark['config']\n",
    "            print(f\"    Config: {config.get('num_fewshot', 0)} shots, limit {config.get('limit', 'unlimited')}\")\n",
    "\n",
    "    if collection.get('metadata'):\n",
    "        metadata = collection['metadata']\n",
    "        print(\"\\nüìä Metadata:\")\n",
    "        print(f\"  Estimated duration: {metadata.get('estimated_duration_minutes', 'unknown')} minutes\")\n",
    "        print(f\"  Difficulty: {metadata.get('difficulty', 'unknown')}\")\n",
    "        print(f\"  Use case: {metadata.get('use_case', 'unknown')}\")\n",
    "elif response.status_code == 404:\n",
    "    print(f\"‚ùå Collection '{collection_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection-Based Evaluations\n",
    "\n",
    "### Execute Evaluation Using a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation request using our coding & reasoning collection\n",
    "# Now with native collection_id support in eval-hub!\n",
    "\n",
    "collection_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": f\"Coding & Reasoning Collection Evaluation - {coding_reasoning_collection_id}\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"TinyLlama Coding & Reasoning\",\n",
    "            \"description\": f\"Evaluation using {coding_reasoning_collection_id} collection with automatic expansion\",\n",
    "            \"model\": {\n",
    "                \"server\": \"vllm\",  # Use the vLLM server from our Kubernetes setup\n",
    "                \"name\": \"tinyllama\",\n",
    "                \"configuration\": {\n",
    "                    \"temperature\": 0.0,  # Deterministic for benchmarking\n",
    "                    \"max_tokens\": 512,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            },\n",
    "            \"collection_id\": coding_reasoning_collection_id,  # ‚ú® Native collection support!\n",
    "            \"timeout_minutes\": 60,  # Allow more time for collection execution\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"evaluation_type\": \"collection\",\n",
    "        \"collection_id\": coding_reasoning_collection_id,\n",
    "        \"model_family\": \"llama\",\n",
    "        \"evaluation_scope\": \"coding_reasoning\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating collection-based evaluation...\")\n",
    "print(f\"Collection ID: {coding_reasoning_collection_id}\")\n",
    "print(\"‚ú® Using native collection_id support - automatic expansion!\")\n",
    "\n",
    "print_json(collection_evaluation)\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=collection_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    collection_eval_response = response.json()\n",
    "    collection_request_id = collection_eval_response[\"request_id\"]\n",
    "    print(\"‚úÖ Collection evaluation created successfully!\")\n",
    "    print(f\"Request ID: {collection_request_id}\")\n",
    "    print(f\"Status: {collection_eval_response['status']}\")\n",
    "    print(f\"Experiment URL: {collection_eval_response.get('experiment_url', 'N/A')}\")\n",
    "\n",
    "    # Automatic collection expansion process:\n",
    "    # 1. ‚úÖ Eval-hub automatically looks up the collection by ID\n",
    "    # 2. ‚úÖ Extracts all benchmarks from the collection\n",
    "    # 3. ‚úÖ Groups benchmarks by provider\n",
    "    # 4. ‚úÖ Creates appropriate backend configurations\n",
    "    # 5. üîÑ Will execute with proper weights and configurations\n",
    "    print(\"\\n‚ú® Native Collection Processing:\")\n",
    "    print(f\"  ‚úÖ Collection ID: {coding_reasoning_collection_id}\")\n",
    "    print(\"  ‚úÖ Automatic backend expansion by eval-hub\")\n",
    "    print(\"  ‚úÖ Benchmark configs and weights preserved\")\n",
    "    print(\"  üîÑ Execution: Creating CR and running evaluation\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create collection evaluation\")\n",
    "    print(f\"Error: {response.text}\")\n",
    "    collection_request_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Multiple Collections in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluations for both collections to compare different reasoning approaches\n",
    "collections_comparison = []\n",
    "\n",
    "for collection_id, collection_name in [\n",
    "    (coding_reasoning_collection_id, \"Coding & Reasoning\"),\n",
    "    (language_collection_id, \"Language Understanding\")\n",
    "]:\n",
    "    comparison_eval = {\n",
    "        \"request_id\": str(uuid4()),\n",
    "        \"experiment_name\": f\"{collection_name} Collection - Model Comparison\",\n",
    "        \"evaluations\": [\n",
    "            {\n",
    "                \"name\": f\"TinyLlama {collection_name} Evaluation\",\n",
    "                \"description\": f\"Comparative evaluation using {collection_id} collection\",\n",
    "                \"model\": {\n",
    "                    \"server\": \"vllm\",\n",
    "                    \"name\": \"tinyllama\",\n",
    "                    \"configuration\": {\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"max_tokens\": 512\n",
    "                    }\n",
    "                },\n",
    "                \"collection_id\": collection_id,  # Just reference the collection!\n",
    "                \"timeout_minutes\": 90,\n",
    "                \"retry_attempts\": 1\n",
    "            }\n",
    "        ],\n",
    "        \"tags\": {\n",
    "            \"evaluation_type\": \"collection_comparison\",\n",
    "            \"collection_id\": collection_id,\n",
    "            \"comparison_group\": \"coding_vs_language\",\n",
    "            \"model\": \"tinyllama\"\n",
    "        }\n",
    "    }\n",
    "    collections_comparison.append(comparison_eval)\n",
    "\n",
    "print(f\"üì¶ Creating {len(collections_comparison)} collection comparison evaluations...\")\n",
    "\n",
    "comparison_request_ids = []\n",
    "for i, eval_request in enumerate(collections_comparison):\n",
    "    collection_name = [\"Coding & Reasoning\", \"Language Understanding\"][i]\n",
    "    print(f\"\\nüìù Creating {collection_name} collection evaluation...\")\n",
    "\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=eval_request)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        comparison_response = response.json()\n",
    "        comparison_request_ids.append(comparison_response[\"request_id\"])\n",
    "        print(f\"‚úÖ {collection_name} evaluation created: {comparison_response['request_id']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to create {collection_name} evaluation\")\n",
    "\n",
    "print(f\"\\nüìä Created {len(comparison_request_ids)} collection comparisons\")\n",
    "for i, req_id in enumerate(comparison_request_ids):\n",
    "    collection_type = [\"Coding & Reasoning\", \"Language Understanding\"][i]\n",
    "    print(f\"  - {collection_type}: {req_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Results Management\n",
    "\n",
    "### Monitor Collection Evaluation Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function specifically for monitoring collection-based evaluations\n",
    "def monitor_collection_evaluation(request_id: str, collection_id: str):\n",
    "    \"\"\"Monitor a collection-based evaluation with collection-specific details.\"\"\"\n",
    "    print(f\"üîç Monitoring collection evaluation: {collection_id}\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        status_data = response.json()\n",
    "        print(\"\\nüìä Collection Evaluation Status:\")\n",
    "        print(f\"Status: {status_data['status']}\")\n",
    "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
    "\n",
    "        # Collection-specific information\n",
    "        if status_data.get('collection_id'):\n",
    "            print(f\"Collection: {status_data['collection_id']}\")\n",
    "\n",
    "        # Show benchmark-level progress if available\n",
    "        if status_data.get('results'):\n",
    "            print(f\"\\nüìã Benchmark Progress ({len(status_data['results'])} completed):\")\n",
    "            for result in status_data['results']:\n",
    "                benchmark_name = result.get('benchmark_name', 'unknown')\n",
    "                result_status = result.get('status', 'unknown')\n",
    "                print(f\"  - {benchmark_name}: {result_status}\")\n",
    "\n",
    "                # Show key metrics if available\n",
    "                if result.get('metrics'):\n",
    "                    metrics = result['metrics']\n",
    "                    key_metrics = []\n",
    "                    for metric_name, metric_value in list(metrics.items())[:2]:  # Show first 2 metrics\n",
    "                        if isinstance(metric_value, (int, float)):\n",
    "                            key_metrics.append(f\"{metric_name}: {metric_value:.3f}\")\n",
    "                    if key_metrics:\n",
    "                        print(f\"    Metrics: {', '.join(key_metrics)}\")\n",
    "\n",
    "        return status_data\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to get collection evaluation status: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Monitor our collection evaluation if it exists\n",
    "if 'collection_request_id' in locals() and collection_request_id:\n",
    "    monitor_collection_evaluation(collection_request_id, coding_reasoning_collection_id)\n",
    "else:\n",
    "    print(\"No active collection evaluation to monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Complete Collection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get comprehensive collection results\n",
    "def get_collection_results(request_id: str, format_for_analysis: bool = True):\n",
    "    \"\"\"\n",
    "    Retrieve and format results for a collection-based evaluation.\n",
    "\n",
    "    Args:\n",
    "        request_id: The evaluation request ID\n",
    "        format_for_analysis: Whether to format results for analysis\n",
    "    \"\"\"\n",
    "    print(\"üìä Retrieving collection evaluation results...\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to retrieve results: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    eval_data = response.json()\n",
    "\n",
    "    if eval_data.get('status') != 'completed':\n",
    "        print(f\"‚è≥ Evaluation not completed. Status: {eval_data.get('status')}\")\n",
    "        return None\n",
    "\n",
    "    collection_id = eval_data.get('collection_id')\n",
    "    print(\"‚úÖ Collection evaluation completed!\")\n",
    "    print(f\"Collection ID: {collection_id}\")\n",
    "    print(f\"Total benchmarks: {len(eval_data.get('results', []))}\")\n",
    "\n",
    "    # Aggregate collection-level metrics\n",
    "    results = eval_data.get('results', [])\n",
    "\n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return eval_data\n",
    "\n",
    "    print(\"\\nüìã Collection Results Summary:\")\n",
    "\n",
    "    # Calculate weighted average scores based on collection benchmark weights\n",
    "    total_weighted_score = 0\n",
    "    total_weight = 0\n",
    "    benchmark_scores = {}\n",
    "\n",
    "    for result in results:\n",
    "        benchmark_name = result.get('benchmark_name', 'unknown')\n",
    "        benchmark_status = result.get('status', 'unknown')\n",
    "\n",
    "        print(f\"\\n  üìä {benchmark_name}: {benchmark_status}\")\n",
    "\n",
    "        if result.get('metrics'):\n",
    "            metrics = result['metrics']\n",
    "\n",
    "            # Extract primary accuracy metric (common across lm-eval-harness benchmarks)\n",
    "            primary_score = None\n",
    "            for metric_name in ['acc', 'acc_norm', 'exact_match', 'score']:\n",
    "                if metric_name in metrics:\n",
    "                    primary_score = metrics[metric_name]\n",
    "                    break\n",
    "\n",
    "            if primary_score is not None:\n",
    "                if isinstance(primary_score, dict) and 'value' in primary_score:\n",
    "                    score_value = primary_score['value']\n",
    "                else:\n",
    "                    score_value = primary_score\n",
    "\n",
    "                benchmark_scores[benchmark_name] = score_value\n",
    "                print(f\"    Primary score: {score_value:.3f}\")\n",
    "\n",
    "                # Get benchmark weight from collection (default 1.0)\n",
    "                weight = 1.0  # Default weight\n",
    "                # Note: In a real implementation, you'd look up the weight from the collection definition\n",
    "\n",
    "                total_weighted_score += score_value * weight\n",
    "                total_weight += weight\n",
    "\n",
    "            # Show additional metrics\n",
    "            other_metrics = []\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if metric_name not in ['acc', 'acc_norm', 'exact_match', 'score']:\n",
    "                    if isinstance(metric_value, (int, float)):\n",
    "                        other_metrics.append(f\"{metric_name}: {metric_value:.3f}\")\n",
    "                    elif isinstance(metric_value, dict) and 'value' in metric_value:\n",
    "                        other_metrics.append(f\"{metric_name}: {metric_value['value']:.3f}\")\n",
    "\n",
    "            if other_metrics:\n",
    "                print(f\"    Other metrics: {', '.join(other_metrics[:3])}\")  # Show first 3\n",
    "\n",
    "    # Calculate collection-level aggregate score\n",
    "    if total_weight > 0:\n",
    "        collection_avg_score = total_weighted_score / total_weight\n",
    "        print(f\"\\nüéØ Collection Aggregate Score: {collection_avg_score:.3f}\")\n",
    "        print(f\"   (Weighted average across {len(benchmark_scores)} benchmarks)\")\n",
    "\n",
    "    if format_for_analysis:\n",
    "        # Format results for further analysis\n",
    "        analysis_format = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"evaluation_id\": request_id,\n",
    "            \"status\": eval_data['status'],\n",
    "            \"completed_at\": eval_data.get('updated_at'),\n",
    "            \"aggregate_score\": collection_avg_score if 'collection_avg_score' in locals() else None,\n",
    "            \"benchmark_scores\": benchmark_scores,\n",
    "            \"benchmark_count\": len(results),\n",
    "            \"successful_benchmarks\": len([r for r in results if r.get('status') == 'completed']),\n",
    "            \"failed_benchmarks\": len([r for r in results if r.get('status') == 'failed']),\n",
    "            \"raw_results\": results\n",
    "        }\n",
    "\n",
    "        print(\"\\nüìã Analysis Format Summary:\")\n",
    "        print(f\"  Successful: {analysis_format['successful_benchmarks']}/{analysis_format['benchmark_count']}\")\n",
    "        print(f\"  Success rate: {(analysis_format['successful_benchmarks']/analysis_format['benchmark_count']*100):.1f}%\")\n",
    "\n",
    "        return analysis_format\n",
    "\n",
    "    return eval_data\n",
    "\n",
    "# Example usage with a completed evaluation\n",
    "if 'collection_request_id' in locals() and collection_request_id:\n",
    "    print(f\"üìä Attempting to retrieve results for: {collection_request_id}\")\n",
    "    collection_results = get_collection_results(collection_request_id)\n",
    "else:\n",
    "    print(\"üìù No collection evaluation request ID available for result retrieval\")\n",
    "    print(\"üìñ Example of what collection results would look like:\")\n",
    "\n",
    "    # Show example collection results structure\n",
    "    example_collection_results = {\n",
    "        \"collection_id\": \"academic_reasoning_v1\",\n",
    "        \"evaluation_id\": \"12345678-1234-1234-1234-123456789012\",\n",
    "        \"status\": \"completed\",\n",
    "        \"aggregate_score\": 0.742,\n",
    "        \"benchmark_scores\": {\n",
    "            \"arc_easy\": 0.753,\n",
    "            \"arc_challenge\": 0.462,\n",
    "            \"hellaswag\": 0.789,\n",
    "            \"mmlu\": 0.654\n",
    "        },\n",
    "        \"benchmark_count\": 4,\n",
    "        \"successful_benchmarks\": 4,\n",
    "        \"failed_benchmarks\": 0\n",
    "    }\n",
    "\n",
    "    print_json(example_collection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Collection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare results across multiple collection evaluations\n",
    "def compare_collection_results(request_ids: list, collection_names: list = None):\n",
    "    \"\"\"Compare results across multiple collection evaluations.\"\"\"\n",
    "\n",
    "    if collection_names is None:\n",
    "        collection_names = [f\"Collection {i+1}\" for i in range(len(request_ids))]\n",
    "\n",
    "    print(f\"üìä Comparing {len(request_ids)} collection evaluations...\")\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for i, request_id in enumerate(request_ids):\n",
    "        collection_name = collection_names[i]\n",
    "        print(f\"\\nüîç Retrieving results for {collection_name}...\")\n",
    "\n",
    "        results = get_collection_results(request_id, format_for_analysis=True)\n",
    "\n",
    "        if results:\n",
    "            comparison_data.append({\n",
    "                \"name\": collection_name,\n",
    "                \"request_id\": request_id,\n",
    "                \"collection_id\": results.get('collection_id'),\n",
    "                \"aggregate_score\": results.get('aggregate_score'),\n",
    "                \"benchmark_scores\": results.get('benchmark_scores', {}),\n",
    "                \"success_rate\": results.get('successful_benchmarks', 0) / max(results.get('benchmark_count', 1), 1),\n",
    "                \"benchmark_count\": results.get('benchmark_count', 0)\n",
    "            })\n",
    "\n",
    "    if not comparison_data:\n",
    "        print(\"‚ùå No valid results to compare\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüìä Collection Performance Comparison:\")\n",
    "    print(f\"{'Collection':<25} {'Aggregate':<10} {'Success Rate':<12} {'Benchmarks':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for data in comparison_data:\n",
    "        aggregate = f\"{data['aggregate_score']:.3f}\" if data['aggregate_score'] else \"N/A\"\n",
    "        success_rate = f\"{data['success_rate']*100:.1f}%\" if data['success_rate'] else \"N/A\"\n",
    "        benchmarks = str(data['benchmark_count'])\n",
    "\n",
    "        print(f\"{data['name']:<25} {aggregate:<10} {success_rate:<12} {benchmarks:<10}\")\n",
    "\n",
    "    # Show benchmark-by-benchmark comparison if there are common benchmarks\n",
    "    all_benchmarks = set()\n",
    "    for data in comparison_data:\n",
    "        all_benchmarks.update(data['benchmark_scores'].keys())\n",
    "\n",
    "    if all_benchmarks:\n",
    "        print(\"\\nüìã Benchmark-by-Benchmark Comparison:\")\n",
    "\n",
    "        for benchmark in sorted(all_benchmarks):\n",
    "            print(f\"\\n  {benchmark}:\")\n",
    "            for data in comparison_data:\n",
    "                score = data['benchmark_scores'].get(benchmark)\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"    {data['name']:<20}: {score_str}\")\n",
    "\n",
    "    return comparison_data\n",
    "\n",
    "# Example usage with comparison request IDs\n",
    "if 'comparison_request_ids' in locals() and comparison_request_ids:\n",
    "    collection_comparison = compare_collection_results(\n",
    "        comparison_request_ids,\n",
    "        [\"Coding & Reasoning\", \"Language Understanding\"]\n",
    "    )\n",
    "else:\n",
    "    print(\"üìù No comparison evaluations available\")\n",
    "    print(\"üìñ This would compare performance across different collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Collection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export collection results for external analysis\n",
    "def export_collection_results(results_data: dict, filename: str = None):\n",
    "    \"\"\"Export collection results to JSON file for external analysis.\"\"\"\n",
    "\n",
    "    if filename is None:\n",
    "        collection_id = results_data.get('collection_id', 'unknown')\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"collection_results_{collection_id}_{timestamp}.json\"\n",
    "\n",
    "    # Prepare export format\n",
    "    export_data = {\n",
    "        \"export_metadata\": {\n",
    "            \"export_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S UTC\"),\n",
    "            \"eval_hub_version\": \"v1\",\n",
    "            \"format_version\": \"1.0\"\n",
    "        },\n",
    "        \"collection_evaluation\": results_data\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    import json\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"üíæ Collection results exported to: {filename}\")\n",
    "    print(\"üìä Export contains:\")\n",
    "    print(f\"  - Collection ID: {results_data.get('collection_id', 'N/A')}\")\n",
    "    print(f\"  - Benchmarks: {results_data.get('benchmark_count', 0)}\")\n",
    "    print(f\"  - Aggregate score: {results_data.get('aggregate_score', 'N/A')}\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Example export\n",
    "example_export_data = {\n",
    "    \"collection_id\": \"coding_reasoning_v1\",\n",
    "    \"aggregate_score\": 0.678,\n",
    "    \"benchmark_count\": 4,\n",
    "    \"benchmark_scores\": {\"arc_easy\": 0.753, \"humaneval\": 0.645, \"mbpp\": 0.672, \"bbh\": 0.642}\n",
    "}\n",
    "\n",
    "print(\"üìù Example collection results export:\")\n",
    "export_filename = export_collection_results(example_export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Server Management\n",
    "\n",
    "### List All Model Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/servers\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    servers_data = response.json()\n",
    "    print(f\"Total servers: {servers_data['total_servers']}\")\n",
    "    print(f\"Runtime servers: {len(servers_data.get('runtime_servers', []))}\")\n",
    "\n",
    "    print(\"\\nüìã Model Servers:\")\n",
    "    for server in servers_data.get('servers', []):\n",
    "        print(f\"  - {server['server_id']}\")\n",
    "        print(f\"    Type: {server['server_type']}\")\n",
    "        print(f\"    Base URL: {server['base_url']}\")\n",
    "        print(f\"    Models: {server['model_count']}\")\n",
    "        print(f\"    Status: {server['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Only Active Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/servers\", params={\"include_inactive\": False})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    servers_data = response.json()\n",
    "    print(f\"Active servers: {servers_data['total_servers']}\")\n",
    "    for server in servers_data.get('servers', []):\n",
    "        print(f\"  - {server['server_id']} - {server['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Server by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details for a specific model server\n",
    "server_id = \"vllm\"  # Replace with an actual server ID from your system\n",
    "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    server = response.json()\n",
    "    print(f\"Server ID: {server['server_id']}\")\n",
    "    print(f\"Type: {server['server_type']}\")\n",
    "    print(f\"Base URL: {server['base_url']}\")\n",
    "    print(f\"Status: {server['status']}\")\n",
    "\n",
    "    print(f\"\\nüì¶ Models on this server ({len(server['models'])}):\")\n",
    "    for model in server['models']:\n",
    "        print(f\"  - {model['model_name']}\")\n",
    "        print(f\"    Status: {model['status']}\")\n",
    "        if model.get('description'):\n",
    "            print(f\"    Description: {model['description']}\")\n",
    "\n",
    "    if server.get('tags'):\n",
    "        print(f\"\\nTags: {', '.join(server['tags'])}\")\n",
    "elif response.status_code == 404:\n",
    "    print(f\"‚ùå Server '{server_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model by Server and Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific model by getting the server and finding the model in its models list\n",
    "server_id = \"vllm\"\n",
    "model_name = \"vllm\"  # Replace with actual model name\n",
    "\n",
    "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    server = response.json()\n",
    "    model = None\n",
    "    for m in server['models']:\n",
    "        if m['model_name'] == model_name:\n",
    "            model = m\n",
    "            break\n",
    "\n",
    "    if model:\n",
    "        print(f\"‚úÖ Found model: {model['model_name']}\")\n",
    "        print(f\"   Server: {server['server_id']}\")\n",
    "        print(f\"   Status: {model['status']}\")\n",
    "        if model.get('description'):\n",
    "            print(f\"   Description: {model['description']}\")\n",
    "        if model.get('capabilities'):\n",
    "            print(f\"   Capabilities: {model['capabilities']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Model '{model_name}' not found on server '{server_id}'\")\n",
    "else:\n",
    "    print(f\"‚ùå Server '{server_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a New Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a model server with models\n",
    "new_server = {\n",
    "    \"server_id\": \"groq-server\",\n",
    "    \"server_type\": \"openai-compatible\",\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "    \"api_key_required\": True,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model_name\": \"llama-3.1-70b\",\n",
    "            \"description\": \"Meta's Llama 3.1 70B model\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"groq\", \"llama\", \"70b\"]\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"llama-3.1-8b\",\n",
    "            \"description\": \"Meta's Llama 3.1 8B model\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"groq\", \"llama\", \"8b\"]\n",
    "        }\n",
    "    ],\n",
    "    \"server_config\": {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"timeout\": 60,\n",
    "        \"retry_attempts\": 3\n",
    "    },\n",
    "    \"status\": \"active\",\n",
    "    \"tags\": [\"groq\", \"openai-compatible\", \"fast\"]\n",
    "}\n",
    "\n",
    "print(\"üìù Registering new model server...\")\n",
    "print_json(new_server)\n",
    "\n",
    "response = api_request(\"POST\", \"/servers\", json=new_server)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    registered_server = response.json()\n",
    "    print(\"‚úÖ Model server registered successfully!\")\n",
    "    print(f\"Server ID: {registered_server['server_id']}\")\n",
    "    print(f\"Models: {len(registered_server['models'])}\")\n",
    "    print(f\"Created at: {registered_server.get('created_at', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to register server: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a vLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a vLLM server\n",
    "vllm_server = {\n",
    "    \"server_id\": \"local-vllm\",\n",
    "    \"server_type\": \"vllm\",\n",
    "    \"base_url\": \"http://localhost:8000\",\n",
    "    \"api_key_required\": False,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model_name\": \"llama-2-7b\",\n",
    "            \"description\": \"Llama 2 7B running on local vLLM server\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"vllm\", \"local\", \"llama-2\"]\n",
    "        }\n",
    "    ],\n",
    "    \"status\": \"active\",\n",
    "    \"tags\": [\"vllm\", \"local\"]\n",
    "}\n",
    "\n",
    "print(\"üìù Registering vLLM server...\")\n",
    "response = api_request(\"POST\", \"/servers\", json=vllm_server)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print(f\"‚úÖ vLLM server registered: {response.json()['server_id']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Note: This may fail if the server ID already exists\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Evaluation Examples\n",
    "\n",
    "### Single Benchmark Evaluation from Builtin Provider (Simplified API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run a single benchmark using the simplified API (Llama Stack compatible)\n",
    "provider_id = \"lm_evaluation_harness\"\n",
    "benchmark_id = \"arc_easy\"\n",
    "\n",
    "single_benchmark_request = {\n",
    "    \"model\": {\n",
    "        \"server\": \"vllm\",\n",
    "        \"name\": \"tinyllama\"\n",
    "    },\n",
    "    \"model_configuration\": {\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 512\n",
    "    },\n",
    "    \"timeout_minutes\": 30,\n",
    "    \"retry_attempts\": 1,\n",
    "    \"limit\": 100,  # Limit to 100 samples for faster execution\n",
    "    \"num_fewshot\": 0,\n",
    "    \"experiment_name\": \"Single Benchmark - ARC Easy\",\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"single_benchmark\",\n",
    "        \"provider\": \"lm_evaluation_harness\",\n",
    "        \"benchmark\": \"arc_easy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating single benchmark evaluation request...\")\n",
    "print(f\"Provider ID: {provider_id}\")\n",
    "print(f\"Benchmark ID: {benchmark_id}\")\n",
    "print_json(single_benchmark_request)\n",
    "\n",
    "response = api_request(\"POST\", f\"/evaluations/benchmarks/{provider_id}/{benchmark_id}\", json=single_benchmark_request)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    evaluation_response = response.json()\n",
    "    request_id = evaluation_response[\"request_id\"]\n",
    "    print(\"‚úÖ Single benchmark evaluation created successfully!\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    print(f\"Status: {evaluation_response['status']}\")\n",
    "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create evaluation\")\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Evaluation with Risk Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple evaluation request using risk category\n",
    "evaluation_request = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Simple Risk-Based Evaluation\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"GPT-4 Mini Low Risk Evaluation\",\n",
    "            \"description\": \"Basic evaluation using low risk benchmarks\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 512\n",
    "            },\n",
    "            \"risk_category\": \"low\",\n",
    "            \"timeout_minutes\": 30,\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"risk_category\",\n",
    "        \"complexity\": \"simple\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating evaluation request...\")\n",
    "print_json(evaluation_request)\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=evaluation_request)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    evaluation_response = response.json()\n",
    "    request_id = evaluation_response[\"request_id\"]\n",
    "    print(\"‚úÖ Evaluation created successfully!\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    print(f\"Status: {evaluation_response['status']}\")\n",
    "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create evaluation\")\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Explicit Backend Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation with explicit backend configuration\n",
    "explicit_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Explicit Backend Configuration\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"LM-Eval Harness Evaluation\",\n",
    "            \"description\": \"Evaluation with explicit lm-evaluation-harness configuration\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"lm-eval-backend\",\n",
    "                    \"type\": \"lm-evaluation-harness\",\n",
    "                    \"config\": {\n",
    "                        \"batch_size\": 1,\n",
    "                        \"device\": \"cpu\"\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"arc_easy\",\n",
    "                            \"tasks\": [\"arc_easy\"],\n",
    "                            \"config\": {\n",
    "                                \"num_fewshot\": 5,\n",
    "                                \"limit\": 50\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"hellaswag\",\n",
    "                            \"tasks\": [\"hellaswag\"],\n",
    "                            \"config\": {\n",
    "                                \"num_fewshot\": 10,\n",
    "                                \"limit\": 100\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 45,\n",
    "            \"retry_attempts\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"explicit_backend\",\n",
    "        \"complexity\": \"intermediate\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating evaluation with explicit backend...\")\n",
    "response = api_request(\"POST\", \"/evaluations\", json=explicit_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    explicit_response = response.json()\n",
    "    explicit_request_id = explicit_response[\"request_id\"]\n",
    "    print(\"‚úÖ Explicit evaluation created!\")\n",
    "    print(f\"Request ID: {explicit_request_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo Evaluator Integration\n",
    "\n",
    "### Single NeMo Evaluator Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with single NeMo Evaluator container\n",
    "nemo_single_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"NeMo Evaluator Single Container\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"GPT-4 via NeMo Evaluator\",\n",
    "            \"description\": \"Remote evaluation using NeMo Evaluator container\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 512,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"remote-nemo-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"localhost\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.openai.com/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"OPENAI_API_KEY\",\n",
    "                        \"timeout_seconds\": 1800,\n",
    "                        \"max_retries\": 2,\n",
    "                        \"verify_ssl\": False,\n",
    "                        \"framework_name\": \"eval-hub-example\",\n",
    "                        \"parallelism\": 1,\n",
    "                        \"limit_samples\": 25,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.95\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"mmlu_pro_sample\",\n",
    "                            \"tasks\": [\"mmlu_pro\"],\n",
    "                            \"config\": {\n",
    "                                \"limit\": 25,\n",
    "                                \"num_fewshot\": 5\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 60,\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"nemo_evaluator_single\",\n",
    "        \"complexity\": \"advanced\",\n",
    "        \"backend\": \"remote_container\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating NeMo Evaluator evaluation...\")\n",
    "print(\"Note: This requires a running NeMo Evaluator container on localhost:3825\")\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=nemo_single_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    nemo_response = response.json()\n",
    "    nemo_request_id = nemo_response[\"request_id\"]\n",
    "    print(\"‚úÖ NeMo evaluation created!\")\n",
    "    print(f\"Request ID: {nemo_request_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NeMo evaluation failed (container may not be running)\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Container NeMo Evaluator Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with multiple specialized NeMo Evaluator containers\n",
    "nemo_multi_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Multi-Container NeMo Evaluation\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"Distributed LLaMA Evaluation\",\n",
    "            \"description\": \"Multi-container evaluation across specialized endpoints\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 512,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"academic-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"academic-eval.example.com\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
    "                        \"timeout_seconds\": 3600,\n",
    "                        \"framework_name\": \"eval-hub-academic\",\n",
    "                        \"parallelism\": 2\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"mmlu_pro\",\n",
    "                            \"tasks\": [\"mmlu_pro\"],\n",
    "                            \"config\": {\"limit\": 100, \"num_fewshot\": 5}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"arc_challenge\",\n",
    "                            \"tasks\": [\"arc_challenge\"],\n",
    "                            \"config\": {\"limit\": 200, \"num_fewshot\": 25}\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"math-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"math-eval.example.com\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"parallelism\": 1,\n",
    "                        \"framework_name\": \"eval-hub-math\"\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"gsm8k\",\n",
    "                            \"tasks\": [\"gsm8k\"],\n",
    "                            \"config\": {\"limit\": 100, \"num_fewshot\": 8}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"math\",\n",
    "                            \"tasks\": [\"hendrycks_math\"],\n",
    "                            \"config\": {\"limit\": 50, \"num_fewshot\": 4}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 120,\n",
    "            \"retry_attempts\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"nemo_evaluator_multi\",\n",
    "        \"complexity\": \"expert\",\n",
    "        \"backend\": \"distributed_containers\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìù Creating multi-container NeMo evaluation...\")\n",
    "print(\"Note: This is a hypothetical example with multiple remote containers\")\n",
    "print_json(nemo_multi_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Status Monitoring\n",
    "\n",
    "### Check Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check evaluation status\n",
    "def check_evaluation_status(request_id: str):\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        status_data = response.json()\n",
    "        print(f\"üìä Evaluation Status for {request_id}\")\n",
    "        print(f\"Status: {status_data['status']}\")\n",
    "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
    "        print(f\"Total evaluations: {status_data.get('total_evaluations', 0)}\")\n",
    "        print(f\"Completed: {status_data.get('completed_evaluations', 0)}\")\n",
    "        print(f\"Failed: {status_data.get('failed_evaluations', 0)}\")\n",
    "\n",
    "        if status_data.get('results'):\n",
    "            print(f\"Results available: {len(status_data['results'])}\")\n",
    "\n",
    "        return status_data\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to get status: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Check status of previously created evaluations (if they exist)\n",
    "try:\n",
    "    if 'request_id' in locals():\n",
    "        check_evaluation_status(request_id)\n",
    "except NameError:\n",
    "    print(\"No evaluation request_id available to check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Evaluation Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor evaluation until completion\n",
    "def monitor_evaluation(request_id: str, max_wait_time: int = 300):\n",
    "    \"\"\"Monitor an evaluation until completion or timeout.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        status_data = check_evaluation_status(request_id)\n",
    "\n",
    "        if not status_data:\n",
    "            break\n",
    "\n",
    "        status = status_data['status']\n",
    "\n",
    "        if status in ['completed', 'failed', 'cancelled']:\n",
    "            print(f\"üèÅ Evaluation {status}!\")\n",
    "\n",
    "            if status == 'completed' and status_data.get('results'):\n",
    "                print(\"\\nüìä Results Summary:\")\n",
    "                for result in status_data['results'][:3]:  # Show first 3 results\n",
    "                    print(f\"  - {result['benchmark_name']}: {result['status']}\")\n",
    "                    if result.get('metrics'):\n",
    "                        for metric, value in list(result['metrics'].items())[:2]:\n",
    "                            print(f\"    {metric}: {value}\")\n",
    "\n",
    "            return status_data\n",
    "\n",
    "        print(f\"‚è≥ Still {status}, waiting...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(f\"‚è∞ Monitoring timed out after {max_wait_time} seconds\")\n",
    "    return None\n",
    "\n",
    "# Example usage (uncomment if you have a running evaluation)\n",
    "# monitor_evaluation(request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/evaluations\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    evaluations = response.json()\n",
    "    print(f\"üìã Active evaluations: {len(evaluations)}\")\n",
    "\n",
    "    for eval_resp in evaluations:\n",
    "        print(f\"\\nüîç {eval_resp['request_id']}\")\n",
    "        print(f\"   Status: {eval_resp['status']}\")\n",
    "        print(f\"   Progress: {eval_resp.get('progress_percentage', 0):.1f}%\")\n",
    "        print(f\"   Created: {eval_resp['created_at']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/metrics/system\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    metrics = response.json()\n",
    "    print(\"üìä System Metrics:\")\n",
    "    print(f\"  Active evaluations: {metrics['active_evaluations']}\")\n",
    "    print(f\"  Running tasks: {metrics['running_tasks']}\")\n",
    "    print(f\"  Total requests: {metrics['total_requests']}\")\n",
    "\n",
    "    if metrics.get('status_breakdown'):\n",
    "        print(\"\\n  Status breakdown:\")\n",
    "        for status, count in metrics['status_breakdown'].items():\n",
    "            print(f\"    {status}: {count}\")\n",
    "\n",
    "    if metrics.get('memory_usage'):\n",
    "        print(\"\\n  Memory usage:\")\n",
    "        print(f\"    Active evaluations: {metrics['memory_usage']['active_evaluations_mb']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Management\n",
    "\n",
    "### Cancel an Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cancel an evaluation\n",
    "def cancel_evaluation(request_id: str):\n",
    "    response = api_request(\"DELETE\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ {result['message']}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to cancel: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment if you want to cancel an evaluation)\n",
    "# cancel_evaluation(request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Examples\n",
    "\n",
    "### Invalid Request Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of invalid request to demonstrate error handling\n",
    "invalid_request = {\n",
    "    \"request_id\": \"invalid-uuid-format\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"\",  # Invalid: empty name\n",
    "            \"model\": {\n",
    "                \"server\": \"\",  # Invalid: empty server\n",
    "                \"name\": \"\"  # Invalid: empty model name\n",
    "            },\n",
    "            \"backends\": []  # Invalid: no backends\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìù Testing error handling with invalid request...\")\n",
    "response = api_request(\"POST\", \"/evaluations\", json=invalid_request)\n",
    "\n",
    "if response.status_code >= 400:\n",
    "    print(\"‚úÖ Error handling working correctly\")\n",
    "    error_data = response.json()\n",
    "    print(f\"Error type: {response.status_code}\")\n",
    "    print(f\"Error message: {error_data.get('detail', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-existent Resource Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accessing non-existent evaluation\n",
    "fake_request_id = str(uuid4())\n",
    "print(f\"üîç Testing access to non-existent evaluation: {fake_request_id}\")\n",
    "\n",
    "response = api_request(\"GET\", f\"/evaluations/{fake_request_id}\")\n",
    "\n",
    "if response.status_code == 404:\n",
    "    print(\"‚úÖ 404 handling working correctly\")\n",
    "    error_data = response.json()\n",
    "    print(f\"Error: {error_data['detail']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Examples\n",
    "\n",
    "### Batch Evaluation Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple evaluations for comparison\n",
    "batch_requests = []\n",
    "\n",
    "models_to_compare = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "risk_levels = [\"low\", \"medium\"]\n",
    "\n",
    "for model in models_to_compare:\n",
    "    for risk in risk_levels:\n",
    "        batch_request = {\n",
    "            \"request_id\": str(uuid4()),\n",
    "            \"experiment_name\": f\"Batch Comparison - {model} - {risk} risk\",\n",
    "            \"evaluations\": [\n",
    "                {\n",
    "                    \"name\": f\"{model} {risk} risk evaluation\",\n",
    "                    \"model\": {\n",
    "                        \"server\": \"default\",\n",
    "                        \"name\": \"default\"\n",
    "                    },\n",
    "                    \"model_configuration\": {\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"max_tokens\": 256\n",
    "                    },\n",
    "                    \"risk_category\": risk,\n",
    "                    \"timeout_minutes\": 30\n",
    "                }\n",
    "            ],\n",
    "            \"tags\": {\n",
    "                \"batch_id\": \"model_comparison_001\",\n",
    "                \"model\": model,\n",
    "                \"risk_level\": risk\n",
    "            }\n",
    "        }\n",
    "        batch_requests.append(batch_request)\n",
    "\n",
    "print(f\"üì¶ Creating {len(batch_requests)} batch evaluations...\")\n",
    "\n",
    "batch_results = []\n",
    "for i, request in enumerate(batch_requests):\n",
    "    print(f\"\\nüìù Creating batch request {i+1}/{len(batch_requests)}\")\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=request)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        batch_results.append(response.json())\n",
    "        print(f\"‚úÖ Batch {i+1} created: {response.json()['request_id']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Batch {i+1} failed\")\n",
    "\n",
    "print(f\"\\nüìä Successfully created {len(batch_results)} batch evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various configuration combinations\n",
    "test_configs = [\n",
    "    {\n",
    "        \"name\": \"High timeout test\",\n",
    "        \"config\": {\"timeout_minutes\": 120, \"retry_attempts\": 5},\n",
    "        \"expected\": \"success\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero timeout test\",\n",
    "        \"config\": {\"timeout_minutes\": 0, \"retry_attempts\": 1},\n",
    "        \"expected\": \"validation_error\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Negative retry test\",\n",
    "        \"config\": {\"timeout_minutes\": 30, \"retry_attempts\": -1},\n",
    "        \"expected\": \"validation_error\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in test_configs:\n",
    "    print(f\"\\nüß™ Testing: {test['name']}\")\n",
    "\n",
    "    test_request = {\n",
    "        \"request_id\": str(uuid4()),\n",
    "        \"experiment_name\": test['name'],\n",
    "        \"evaluations\": [\n",
    "            {\n",
    "                \"name\": \"Config test\",\n",
    "                \"model\": {\n",
    "                    \"server\": \"default\",\n",
    "                    \"name\": \"default\"\n",
    "                },\n",
    "                \"risk_category\": \"low\",\n",
    "                **test['config']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=test_request)\n",
    "\n",
    "    if test['expected'] == \"success\" and response.status_code == 202:\n",
    "        print(\"‚úÖ Test passed\")\n",
    "    elif test['expected'] == \"validation_error\" and response.status_code >= 400:\n",
    "        print(\"‚úÖ Validation correctly rejected invalid config\")\n",
    "    else:\n",
    "        print(f\"‚ùå Unexpected result: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Formatting Results\n",
    "\n",
    "### Get Evaluation Results in NeMo Evaluator Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format eval-hub results to NeMo Evaluator compatible format\n",
    "def format_to_nemo_evaluator(eval_hub_result):\n",
    "    \"\"\"\n",
    "    Convert eval-hub result format to NeMo Evaluator EvaluationResult format.\n",
    "\n",
    "    Expected NeMo format:\n",
    "    {\n",
    "        \"tasks\": {\n",
    "            \"task_name\": {\n",
    "                \"metrics\": {\n",
    "                    \"metric_name\": {\n",
    "                        \"scores\": {\n",
    "                            \"score_name\": {\n",
    "                                \"value\": float,\n",
    "                                \"stats\": {\n",
    "                                    \"count\": int,\n",
    "                                    \"sum\": float,\n",
    "                                    \"mean\": float,\n",
    "                                    \"stderr\": float,\n",
    "                                    \"min\": float,\n",
    "                                    \"max\": float,\n",
    "                                    \"variance\": float,\n",
    "                                    \"stddev\": float\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"groups\": { ... }  # Same structure as tasks\n",
    "    }\n",
    "    \"\"\"\n",
    "    nemo_result = {\n",
    "        \"tasks\": {},\n",
    "        \"groups\": {}\n",
    "    }\n",
    "\n",
    "    # Extract benchmark results from eval-hub format\n",
    "    if 'results' in eval_hub_result:\n",
    "        for result in eval_hub_result['results']:\n",
    "            benchmark_name = result.get('benchmark_name', 'unknown_benchmark')\n",
    "            metrics = result.get('metrics', {})\n",
    "\n",
    "            # Convert metrics to NeMo format\n",
    "            nemo_metrics = {}\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Simple scalar metric\n",
    "                    nemo_metrics[metric_name] = {\n",
    "                        \"scores\": {\n",
    "                            metric_name: {\n",
    "                                \"value\": float(metric_value),\n",
    "                                \"stats\": {\n",
    "                                    \"count\": 1,\n",
    "                                    \"sum\": float(metric_value),\n",
    "                                    \"mean\": float(metric_value),\n",
    "                                    \"stderr\": 0.0,\n",
    "                                    \"min\": float(metric_value),\n",
    "                                    \"max\": float(metric_value),\n",
    "                                    \"variance\": 0.0,\n",
    "                                    \"stddev\": 0.0\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                elif isinstance(metric_value, dict) and 'value' in metric_value:\n",
    "                    # Structured metric with stats\n",
    "                    stats = metric_value.get('stats', {})\n",
    "                    nemo_metrics[metric_name] = {\n",
    "                        \"scores\": {\n",
    "                            metric_name: {\n",
    "                                \"value\": float(metric_value['value']),\n",
    "                                \"stats\": {\n",
    "                                    \"count\": stats.get('count', 1),\n",
    "                                    \"sum\": stats.get('sum', metric_value['value']),\n",
    "                                    \"mean\": stats.get('mean', metric_value['value']),\n",
    "                                    \"stderr\": stats.get('stderr', 0.0),\n",
    "                                    \"min\": stats.get('min', metric_value['value']),\n",
    "                                    \"max\": stats.get('max', metric_value['value']),\n",
    "                                    \"variance\": stats.get('variance', 0.0),\n",
    "                                    \"stddev\": stats.get('stddev', 0.0)\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "            # Add to tasks\n",
    "            nemo_result[\"tasks\"][benchmark_name] = {\"metrics\": nemo_metrics}\n",
    "\n",
    "            # Add to groups (using provider as group name)\n",
    "            provider_id = result.get('provider_id', 'unknown_provider')\n",
    "            if provider_id not in nemo_result[\"groups\"]:\n",
    "                nemo_result[\"groups\"][provider_id] = {\"metrics\": {}}\n",
    "\n",
    "            # Aggregate metrics at group level\n",
    "            for metric_name, metric_data in nemo_metrics.items():\n",
    "                if metric_name not in nemo_result[\"groups\"][provider_id][\"metrics\"]:\n",
    "                    nemo_result[\"groups\"][provider_id][\"metrics\"][metric_name] = metric_data\n",
    "\n",
    "    return nemo_result\n",
    "\n",
    "# Example: Get results for a completed evaluation\n",
    "def get_evaluation_results_nemo_format(request_id: str):\n",
    "    \"\"\"Get evaluation results and format them for NeMo Evaluator compatibility.\"\"\"\n",
    "    print(f\"üîç Retrieving results for evaluation: {request_id}\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to get evaluation results: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    eval_data = response.json()\n",
    "\n",
    "    # Check if evaluation is completed\n",
    "    if eval_data.get('status') != 'completed':\n",
    "        print(f\"‚è≥ Evaluation not completed yet. Status: {eval_data.get('status')}\")\n",
    "        print(f\"üìä Progress: {eval_data.get('progress_percentage', 0):.1f}%\")\n",
    "        return None\n",
    "\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìä Total evaluations: {eval_data.get('total_evaluations', 0)}\")\n",
    "    print(f\"‚úÖ Completed: {eval_data.get('completed_evaluations', 0)}\")\n",
    "    print(f\"‚ùå Failed: {eval_data.get('failed_evaluations', 0)}\")\n",
    "\n",
    "    # Format to NeMo Evaluator structure\n",
    "    nemo_formatted = format_to_nemo_evaluator(eval_data)\n",
    "\n",
    "    print(\"\\nüéØ Results formatted for NeMo Evaluator:\")\n",
    "    print_json(nemo_formatted)\n",
    "\n",
    "    return nemo_formatted\n",
    "\n",
    "# Example usage with a completed evaluation\n",
    "# Replace with an actual request_id from a completed evaluation\n",
    "example_request_id = \"00000000-0000-0000-0000-000000000000\"  # Placeholder\n",
    "\n",
    "print(\"üìù Example: Retrieving evaluation results...\")\n",
    "print(f\"Note: Replace '{example_request_id}' with actual request ID from completed evaluation\")\n",
    "\n",
    "# Simulated example of what the formatted result would look like\n",
    "example_nemo_result = {\n",
    "    \"tasks\": {\n",
    "        \"arc_easy\": {\n",
    "            \"metrics\": {\n",
    "                \"acc\": {\n",
    "                    \"scores\": {\n",
    "                        \"acc\": {\n",
    "                            \"value\": 0.7534,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2376,\n",
    "                                \"sum\": 1790.0,\n",
    "                                \"mean\": 0.7534,\n",
    "                                \"stderr\": 0.0088,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.1856,\n",
    "                                \"stddev\": 0.4307\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"acc_norm\": {\n",
    "                    \"scores\": {\n",
    "                        \"acc_norm\": {\n",
    "                            \"value\": 0.7447,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2376,\n",
    "                                \"sum\": 1769.0,\n",
    "                                \"mean\": 0.7447,\n",
    "                                \"stderr\": 0.0089,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.1902,\n",
    "                                \"stddev\": 0.4361\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"humaneval\": {\n",
    "            \"metrics\": {\n",
    "                \"pass_at_1\": {\n",
    "                    \"scores\": {\n",
    "                        \"pass_at_1\": {\n",
    "                            \"value\": 0.6451,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 164,\n",
    "                                \"sum\": 105.8,\n",
    "                                \"mean\": 0.6451,\n",
    "                                \"stderr\": 0.0374,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.229,\n",
    "                                \"stddev\": 0.4784\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"bleu\": {\n",
    "                    \"scores\": {\n",
    "                        \"bleu\": {\n",
    "                            \"value\": 0.1234,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 164,\n",
    "                                \"sum\": 20.24,\n",
    "                                \"mean\": 0.1234,\n",
    "                                \"stderr\": 0.0156,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.0399,\n",
    "                                \"stddev\": 0.1998\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"groups\": {\n",
    "        \"lm_evaluation_harness\": {\n",
    "            \"metrics\": {\n",
    "                \"avg_score\": {\n",
    "                    \"scores\": {\n",
    "                        \"avg_score\": {\n",
    "                            \"value\": 0.6993,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2,\n",
    "                                \"sum\": 1.3986,\n",
    "                                \"mean\": 0.6993,\n",
    "                                \"stderr\": 0.0542,\n",
    "                                \"min\": 0.6451,\n",
    "                                \"max\": 0.7534,\n",
    "                                \"variance\": 0.0058,\n",
    "                                \"stddev\": 0.0765\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìÑ Example NeMo Evaluator formatted result:\")\n",
    "print_json(example_nemo_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save NeMo formatted results to file\n",
    "def save_nemo_results(nemo_result, filename=\"nemo_evaluation_results.json\"):\n",
    "    \"\"\"Save NeMo Evaluator formatted results to JSON file.\"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(nemo_result, f, indent=2)\n",
    "\n",
    "    print(f\"üíæ Results saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# save_nemo_results(example_nemo_result, \"coding_reasoning_collection_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate NeMo Format Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate NeMo Evaluator format compatibility\n",
    "def validate_nemo_format(result_dict):\n",
    "    \"\"\"\n",
    "    Validate that the result dictionary conforms to NeMo Evaluator format.\n",
    "\n",
    "    Returns: (is_valid: bool, errors: list)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    # Check top-level structure\n",
    "    if not isinstance(result_dict, dict):\n",
    "        errors.append(\"Result must be a dictionary\")\n",
    "        return False, errors\n",
    "\n",
    "    if \"tasks\" not in result_dict:\n",
    "        errors.append(\"Missing required 'tasks' field\")\n",
    "\n",
    "    if \"groups\" not in result_dict:\n",
    "        errors.append(\"Missing required 'groups' field\")\n",
    "\n",
    "    # Validate tasks structure\n",
    "    if \"tasks\" in result_dict:\n",
    "        tasks = result_dict[\"tasks\"]\n",
    "        if not isinstance(tasks, dict):\n",
    "            errors.append(\"'tasks' must be a dictionary\")\n",
    "        else:\n",
    "            for task_name, task_data in tasks.items():\n",
    "                if not isinstance(task_data, dict):\n",
    "                    errors.append(f\"Task '{task_name}' must be a dictionary\")\n",
    "                    continue\n",
    "\n",
    "                if \"metrics\" not in task_data:\n",
    "                    errors.append(f\"Task '{task_name}' missing 'metrics' field\")\n",
    "                    continue\n",
    "\n",
    "                metrics = task_data[\"metrics\"]\n",
    "                for metric_name, metric_data in metrics.items():\n",
    "                    if \"scores\" not in metric_data:\n",
    "                        errors.append(f\"Metric '{metric_name}' in task '{task_name}' missing 'scores'\")\n",
    "                        continue\n",
    "\n",
    "                    scores = metric_data[\"scores\"]\n",
    "                    for score_name, score_data in scores.items():\n",
    "                        if \"value\" not in score_data:\n",
    "                            errors.append(f\"Score '{score_name}' missing 'value'\")\n",
    "                        if \"stats\" not in score_data:\n",
    "                            errors.append(f\"Score '{score_name}' missing 'stats'\")\n",
    "                        elif not isinstance(score_data[\"stats\"], dict):\n",
    "                            errors.append(f\"Score '{score_name}' stats must be a dictionary\")\n",
    "\n",
    "    is_valid = len(errors) == 0\n",
    "    return is_valid, errors\n",
    "\n",
    "# Validate the example result\n",
    "is_valid, validation_errors = validate_nemo_format(example_nemo_result)\n",
    "\n",
    "print(f\"üîç NeMo format validation: {'‚úÖ Valid' if is_valid else '‚ùå Invalid'}\")\n",
    "if validation_errors:\n",
    "    print(\"Validation errors:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"‚úÖ All format requirements satisfied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive usage of the Eval Hub API including:\n",
    "\n",
    "- ‚úÖ **Basic Operations**: Health checks, provider/benchmark discovery\n",
    "- ‚úÖ **Collection Management**: Create custom collections, list collections, and get detailed collection information\n",
    "- ‚úÖ **Collection-Based Evaluations**: Execute evaluations using collections with automatic provider task aggregation\n",
    "- ‚úÖ **Collection Results**: Monitor collection progress, retrieve aggregate results, and compare collection performance\n",
    "- ‚úÖ **Model Management**: Register, list, update, and delete models\n",
    "- ‚úÖ **Simple Evaluations**: Risk category-based evaluations\n",
    "- ‚úÖ **Advanced Evaluations**: Explicit backend configuration\n",
    "- ‚úÖ **NeMo Integration**: Single and multi-container setups\n",
    "- ‚úÖ **Monitoring**: Status checking and progress tracking\n",
    "- ‚úÖ **Management**: Cancellation and system metrics\n",
    "- ‚úÖ **Error Handling**: Validation and error responses\n",
    "- ‚úÖ **Batch Operations**: Multiple evaluation management\n",
    "- ‚úÖ **Result Formatting**: NeMo Evaluator compatible result transformation and validation\n",
    "\n",
    "For production use, remember to:\n",
    "- Use proper API keys and authentication\n",
    "- Configure appropriate timeouts for your evaluation complexity\n",
    "- Monitor resource usage and system metrics\n",
    "- Handle errors gracefully in your applications\n",
    "- Use the async evaluation mode for long-running evaluations\n",
    "\n",
    "The Eval Hub provides a powerful and flexible API for orchestrating machine learning model evaluations across multiple backends and evaluation frameworks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}